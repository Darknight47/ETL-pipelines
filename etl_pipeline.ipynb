{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Task\n",
    "Scrapping data from a web page and build a Data ETL Pipeline using data collected from the web!  \n",
    "> A pipeline to scrape textual data from any article on the web!\n",
    "\n",
    "1. The process begins with data extraction, where relevant information is collected from websites, APIs, databases, and other online sources.  \n",
    "2. Then this raw data is transformed through various operations, including cleaning, filtering, structuring, and aggregating.  \n",
    "3. After transformation, the data is loaded into a CSV file or database, making it accessible for further analysis, reporting, and decision-making.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Extracting text from any article on the web!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WebScrapper:\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "    \n",
    "    def extract_article_text(self):\n",
    "        response = requests.get(self.url)\n",
    "        html_content = response.content\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        article_text = soup.get_text()\n",
    "        return article_text\n",
    "\"\"\"\n",
    "The class provides a way conveniently extract the main text content of an article from a given web page URL.  \n",
    "By creating an instance of the class and calling the 'extract_article_text' method we can retrieval the textual data of the article.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. As we want to store the frequency of each word in the article, we need to clean and preprocess the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextProcessor:\n",
    "    def __init__(self, nltk_stepwords):\n",
    "        self.nltk_stepwords = nltk_stepwords\n",
    "    \n",
    "    def tokenize_and_clean(self, text):\n",
    "        words = text.split()\n",
    "        filtered_words = [word.lower() for word in words if word.isalpha() and word.lower() not in self.nltk_stepwords]\n",
    "        return filtered_words\n",
    "\"\"\"\n",
    "The class provides a way to process text data by tokenizing it into words and cleaning those words by removing non-alphabetic words and stopwords.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
